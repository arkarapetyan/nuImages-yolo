{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdQE83bRPAPG"
   },
   "source": [
    "# Dataset Creation with nuImages devkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UX6iKl_6o21c"
   },
   "source": [
    "### Mounting Google Drive (Optional, Requires lots of disk space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OcZBuwGRQKW2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "%cd /content/drive/MyDrive/nuImages-yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBEGtQ61PIZE"
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wget\n",
    "%pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "import tarfile\n",
    "import json\n",
    "import yaml\n",
    "from shutil import copyfile, rmtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = './'\n",
    "raw_path = os.path.join(home_path, '/data/raw')\n",
    "data_path = os.path.join(home_path, '/data/sets/nuimages/')\n",
    "dataset_path = os.path.join(home_path, '/datasets/nuImages')\n",
    "if not os.path.exists(raw_path):\n",
    "    os.makedirs(raw_path)\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "if not os.path.exists(dataset_path):\n",
    "    os.makedirs(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Extracting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget.download('https://d36yt3mvayqw5m.cloudfront.net/public/nuimages-v1.0/nuimages-v1.0-all-samples.tgz', out=raw_path)\n",
    "wget.download('https://d36yt3mvayqw5m.cloudfront.net/public/nuimages-v1.0/nuimages-v1.0-all-metadata.tgz', out=raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archives = os.listdir(raw_path)\n",
    "for archive_name in archives:\n",
    "    with tarfile.open(os.path.join(raw_path, archive_name)) as archive:\n",
    "        archive.extractall(path=data_path)\n",
    "    os.remove(os.path.join(raw_path, archive_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'test' # the split of the dataset. Can be train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tables\n",
    "tables = {'sample_data': None, 'object_ann': None, 'category': None}\n",
    "for name in tables.keys():\n",
    "    with open(os.path.join(data_path, f'v1.0-{split}', f'{name}.json')) as table_file:\n",
    "        tables[name] = json.load(table_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the Object Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_indices = {}\n",
    "classes = {}\n",
    "c = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class name given the token\n",
    "def get_class(category_token):\n",
    "    for category in tables['category']:\n",
    "        if category['token'] == category_token:\n",
    "            return category['name']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the classes\n",
    "cat_count = len(tables['category'])\n",
    "for i, category in enumerate(tables['category']):\n",
    "    name = category['name']\n",
    "    print(f'category: {i+1}/{cat_count} name: {name}')\n",
    "    class_indices[name] = i\n",
    "    classes[category['token']] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {}\n",
    "start = 0 # Start index, can be used for dividing the dataset\n",
    "sample_count = 8000 # Number of samples to be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the filename given the input token\n",
    "def get_filename(data_token):\n",
    "    for sample in tables['sample_data']:\n",
    "        if sample['token'] == data_token:\n",
    "    return sample['filename']\n",
    "\n",
    "# Compute the Normalized (Relative) bbox\n",
    "def relative_bbox(raw_bbox, size):\n",
    "    x_center = round((raw_bbox[0] + raw_bbox[2]) / (2 * size[1]), 6)\n",
    "    y_center = round((raw_bbox[1] + raw_bbox[3]) / (2 * size[0]), 6)\n",
    "    width = round((raw_bbox[2] - raw_bbox[0]) / size[1], 6)\n",
    "    height = round((raw_bbox[3] - raw_bbox[1]) / size[0], 6)\n",
    "    if x_center > 1 or y_center > 1 or width < 0 or height < 0:\n",
    "        raise ValueError() #debugging\n",
    "    return [x_center, y_center, width, height]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset dictionary\n",
    "\n",
    "sample_data_list = list(filter(lambda s: s['is_key_frame'], tables['sample_data']))\n",
    "sample_data_list = sample_data_list[start:start+sample_count]\n",
    "sample_count = len(sample_data_list)\n",
    "for i, sample in enumerate(sample_data_list):\n",
    "    print(f'sample {i+1}/{sample_count}', end='')\n",
    "    if not sample['is_key_frame']:\n",
    "        print('not a keyframe')\n",
    "        continue\n",
    "    else:\n",
    "        print('')\n",
    "    token = sample['token']\n",
    "    objects = list(filter(lambda obj: obj['sample_data_token']==token, tables['object_ann']))\n",
    "    labels = []\n",
    "    for obj in objects:\n",
    "        size = obj['mask']['size'] if obj.get('mask', None) else [900, 1600]\n",
    "        bbox = relative_bbox(obj['bbox'], size)\n",
    "        obj_class = classes[obj['category_token']]\n",
    "        obj_label = [obj_class] + bbox\n",
    "        obj_label = ' '.join([str(x) for x in obj_label])\n",
    "        labels.append(obj_label)\n",
    "    samples[token] = {}\n",
    "    samples[token]['filename'] = sample['filename']\n",
    "    samples[token]['label'] = '\\n'.join(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the samples to the files\n",
    "\n",
    "for i, (token, sample) in enumerate(samples.items()):\n",
    "    print(f'sample {i + 1}/{sample_count}')\n",
    "    data_full_path = os.path.join(data_path, sample['filename'])\n",
    "    data_new_path = os.path.join(dataset_path, 'images', split)\n",
    "    if not os.path.exists(data_new_path):\n",
    "        os.makedirs(data_new_path)\n",
    "    copyfile(data_full_path, os.path.join(data_new_path, f'{token}.jpg'))\n",
    "    label_path = os.path.join(dataset_path, 'labels', split)\n",
    "    if not os.path.exists(label_path):\n",
    "        os.makedirs(label_path)\n",
    "    with open(os.path.join(label_path, f'{token}.txt'), 'w') as label_file:\n",
    "        label_file.write(sample['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, archive the obtained samples and delete the folder to repeat the process.\n",
    "\n",
    "archive_path = os.path.join(home_path, '/data/archives/')\n",
    "if not os.path.exists(archive_path):\n",
    "    os.makedirs(archive_path)\n",
    "archive_name = 'nuImages-train-1.tar.gz'\n",
    "with tarfile.open(os.path.join(archive_path, archive_name), \"w:gz\") as archive:\n",
    "    archive.add(dataset_path, arcname=os.path.basename(dataset_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(rm_path):\n",
    "    if os.path.isfile(rm_path) or os.path.islink(rm_path):\n",
    "        os.remove(rm_path)  # remove the file\n",
    "    elif os.path.isdir(rm_path):\n",
    "        rmtree(rm_path)  # remove dir and all contains\n",
    "    else:\n",
    "        raise ValueError(f'file {rm_path} is not a file or dir.')\n",
    "    \n",
    "remove(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YAML Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create COCO format yaml file for the dataset \n",
    "\n",
    "def create_coco_yaml(class_dict, file_name):\n",
    "    categories = {idx: name for name, idx in class_dict.items()}\n",
    "    coco_dict = {\n",
    "        'path': dataset_path,\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'names': categories\n",
    "    }\n",
    "    try:\n",
    "        with open(file_name, 'w') as yaml_file:\n",
    "            yaml.dump(coco_dict, yaml_file, default_flow_style=False)\n",
    "        print(f\"COCO dataset YAML successfully written to {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing COCO dataset to YAML file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_coco_yaml(class_indices, os.path.join(dataset_path, 'data.yaml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archive the yaml file\n",
    "\n",
    "archive_name = 'nuImages-yaml.tar.gz'\n",
    "with tarfile.open(os.path.join(archive_path, archive_name), \"w:gz\") as archive:\n",
    "    archive.add(dataset_path, arcname=os.path.basename(dataset_path))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPbDkcLHBNu14U864K3l1SB",
   "collapsed_sections": [
    "UX6iKl_6o21c",
    "bBEGtQ61PIZE",
    "HCNCymaLPMG0",
    "xHXMfYDpJqFC",
    "BCdC3gzxJ2BR",
    "yG_cC6V1ixwb",
    "JHhttLbdo3bp"
   ],
   "mount_file_id": "1kH4Cu4NLJEV-u1t8SRT2jyKXdPx5igzK",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
