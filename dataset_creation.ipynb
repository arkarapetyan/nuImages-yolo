{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdQE83bRPAPG"
   },
   "source": [
    "# Dataset Creation with nuImages devkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UX6iKl_6o21c"
   },
   "source": [
    "### Mounting Google Drive (Optional, Requires lots of disk space)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%pip install google-colab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OcZBuwGRQKW2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "%cd /content/drive/MyDrive/nuImages-yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBEGtQ61PIZE"
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%pip install wget\n",
    "%pip install pyyaml"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "import tarfile\n",
    "import json\n",
    "import yaml\n",
    "from shutil import copyfile, rmtree"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "home_path = './'\n",
    "raw_path = os.path.join(home_path, '/data/raw')\n",
    "data_path = os.path.join(home_path, '/data/sets/nuimages/')\n",
    "dataset_path = os.path.join(home_path, '/datasets/nuImages')\n",
    "if not os.path.exists(raw_path):\n",
    "  os.makedirs(raw_path)\n",
    "if not os.path.exists(data_path):\n",
    "  os.makedirs(data_path)\n",
    "if not os.path.exists(dataset_path):\n",
    "  os.makedirs(dataset_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Downloading and Extracting the Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "wget.download('https://d36yt3mvayqw5m.cloudfront.net/public/nuimages-v1.0/nuimages-v1.0-all-samples.tgz', out=raw_path)\n",
    "wget.download('https://d36yt3mvayqw5m.cloudfront.net/public/nuimages-v1.0/nuimages-v1.0-all-metadata.tgz', out=raw_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "archives = os.listdir(raw_path)\n",
    "for archive_name in archives:\n",
    "  with tarfile.open(os.path.join(raw_path, archive_name)) as archive:\n",
    "    archive.extractall(path=data_path)\n",
    "  os.remove(os.path.join(raw_path, archive_name))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCNCymaLPMG0"
   },
   "source": [
    "## Loading Dataset Tables"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "split = 'test' # the split of the dataset. Can be train/val/test"
   ],
   "metadata": {
    "id": "ePF9Dlv381AK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIVgf0Tx5CRr"
   },
   "outputs": [],
   "source": [
    "# load the tables\n",
    "tables = {'sample_data': None, 'object_ann': None, 'category': None}\n",
    "for name in tables.keys():\n",
    "  with open(os.path.join(data_path, f'v1.0-{split}', f'{name}.json')) as table_file:\n",
    "    tables[name] = json.load(table_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Obtaining the Object Classes"
   ],
   "metadata": {
    "id": "xHXMfYDpJqFC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class_indices = {}\n",
    "classes = {}\n",
    "c = 0"
   ],
   "metadata": {
    "id": "7dNxT5C_etvs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Get class name given the token\n",
    "def get_class(category_token):\n",
    "  for category in tables['category']:\n",
    "    if category['token'] == category_token:\n",
    "      return category['name']\n",
    "  return None"
   ],
   "metadata": {
    "id": "6ci9G-H17gJH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Adding the classes\n",
    "cat_count = len(tables['category'])\n",
    "for i, category in enumerate(tables['category']):\n",
    "  name = category['name']\n",
    "  print(f'category: {i+1}/{cat_count} name: {name}')\n",
    "  class_indices[name] = i\n",
    "  classes[category['token']] = i"
   ],
   "metadata": {
    "id": "lWzAl5tQewRr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the samples"
   ],
   "metadata": {
    "id": "BCdC3gzxJ2BR"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyh9i12Wj7j9"
   },
   "outputs": [],
   "source": [
    "samples = {}\n",
    "start = 0 # Start index, can be used for dividing the dataset\n",
    "sample_count = 8000 # Number of samples to be created"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Get the filename given the input token\n",
    "def get_filename(data_token):\n",
    "  for sample in tables['sample_data']:\n",
    "    if sample['token'] == data_token:\n",
    "      return sample['filename']\n",
    "\n",
    "# Compute the Normalized (Relative) bbox\n",
    "def relative_bbox(raw_bbox, size):\n",
    "  x_center = round((raw_bbox[0] + raw_bbox[2]) / (2 * size[1]), 6)\n",
    "  y_center = round((raw_bbox[1] + raw_bbox[3]) / (2 * size[0]), 6)\n",
    "  width = round((raw_bbox[2] - raw_bbox[0]) / size[1], 6)\n",
    "  height = round((raw_bbox[3] - raw_bbox[1]) / size[0], 6)\n",
    "  if x_center > 1 or y_center > 1 or width < 0 or height < 0:\n",
    "    raise ValueError() #debugging\n",
    "  return [x_center, y_center, width, height]"
   ],
   "metadata": {
    "id": "hmAeW30Ko5SE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create the dataset dictionary\n",
    "\n",
    "sample_data_list = list(filter(lambda s: s['is_key_frame'], tables['sample_data']))\n",
    "sample_data_list = sample_data_list[start:start+sample_count]\n",
    "sample_count = len(sample_data_list)\n",
    "for i, sample in enumerate(sample_data_list):\n",
    "  print(f'sample {i+1}/{sample_count}', end='')\n",
    "  if not sample['is_key_frame']:\n",
    "    print('not a keyframe')\n",
    "    continue\n",
    "  else:\n",
    "    print('')\n",
    "  token = sample['token']\n",
    "  objects = list(filter(lambda obj: obj['sample_data_token']==token, tables['object_ann']))\n",
    "  labels = []\n",
    "  for obj in objects:\n",
    "    size = obj['mask']['size'] if obj.get('mask', None) else [900, 1600]\n",
    "    bbox = relative_bbox(obj['bbox'], size)\n",
    "    obj_class = classes[obj['category_token']]\n",
    "    obj_label = [obj_class] + bbox\n",
    "    obj_label = ' '.join([str(x) for x in obj_label])\n",
    "    labels.append(obj_label)\n",
    "  samples[token] = {}\n",
    "  samples[token]['filename'] = sample['filename']\n",
    "  samples[token]['label'] = '\\n'.join(labels)\n"
   ],
   "metadata": {
    "id": "e4x3apKLe3qb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOlbXS7rC8qm"
   },
   "outputs": [],
   "source": [
    "# Save the samples to the files\n",
    "\n",
    "for i, (token, sample) in enumerate(samples.items()):\n",
    "  print(f'sample {i + 1}/{sample_count}')\n",
    "  data_full_path = os.path.join(data_path, sample['filename'])\n",
    "  data_new_path = os.path.join(dataset_path, 'images', split)\n",
    "  if not os.path.exists(data_new_path):\n",
    "    os.makedirs(data_new_path)\n",
    "  copyfile(data_full_path, os.path.join(data_new_path, f'{token}.jpg'))\n",
    "\n",
    "  label_path = os.path.join(dataset_path, 'labels', split)\n",
    "  if not os.path.exists(label_path):\n",
    "    os.makedirs(label_path)\n",
    "  with open(os.path.join(label_path, f'{token}.txt'), 'w') as label_file:\n",
    "    label_file.write(sample['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMKkkZ4J8LCH"
   },
   "outputs": [],
   "source": [
    "# Optionally, archive the obtained samples and delete the folder to repeat the process.\n",
    "\n",
    "archive_path = os.path.join(home_path, '/data/archives/')\n",
    "if not os.path.exists(archive_path):\n",
    "  os.makedirs(archive_path)\n",
    "archive_name = 'nuImages-train-1.tar.gz'\n",
    "with tarfile.open(os.path.join(archive_path, archive_name), \"w:gz\") as archive:\n",
    "  archive.add(dataset_path, arcname=os.path.basename(dataset_path))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def remove(rm_path):\n",
    "    if os.path.isfile(rm_path) or os.path.islink(rm_path):\n",
    "        os.remove(rm_path)  # remove the file\n",
    "    elif os.path.isdir(rm_path):\n",
    "        rmtree(rm_path)  # remove dir and all contains\n",
    "    else:\n",
    "        raise ValueError(f'file {rm_path} is not a file or dir.')\n",
    "    \n",
    "remove(dataset_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## YAML Creation"
   ],
   "metadata": {
    "id": "yG_cC6V1ixwb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create COCO format yaml file for the dataset \n",
    "\n",
    "def create_coco_yaml(class_dict, file_name):\n",
    "    categories = {idx: name for name, idx in class_dict.items()}\n",
    "    coco_dict = {\n",
    "        'path': dataset_path,\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'names': categories\n",
    "    }\n",
    "    try:\n",
    "        with open(file_name, 'w') as yaml_file:\n",
    "            yaml.dump(coco_dict, yaml_file, default_flow_style=False)\n",
    "        print(f\"COCO dataset YAML successfully written to {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing COCO dataset to YAML file: {e}\")"
   ],
   "metadata": {
    "id": "JThCon4si1s1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "create_coco_yaml(class_indices, os.path.join(dataset_path, 'data.yaml'))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_mwnfUFYojzm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1716059200517,
     "user_tz": -240,
     "elapsed": 5,
     "user": {
      "displayName": "Aren Karapetyan",
      "userId": "12074025844959044397"
     }
    },
    "outputId": "449c6180-aeaf-4119-adb6-e02ab05ec8d8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Archive the yaml file\n",
    "\n",
    "archive_name = 'nuImages-yaml.tar.gz'\n",
    "with tarfile.open(os.path.join(archive_path, archive_name), \"w:gz\") as archive:\n",
    "  archive.add(dataset_path, arcname=os.path.basename(dataset_path))"
   ],
   "metadata": {
    "id": "S8sqJJBRrMK2"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "UX6iKl_6o21c",
    "bBEGtQ61PIZE",
    "HCNCymaLPMG0",
    "xHXMfYDpJqFC",
    "BCdC3gzxJ2BR",
    "yG_cC6V1ixwb",
    "JHhttLbdo3bp"
   ],
   "mount_file_id": "1kH4Cu4NLJEV-u1t8SRT2jyKXdPx5igzK",
   "authorship_tag": "ABX9TyPbDkcLHBNu14U864K3l1SB"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
